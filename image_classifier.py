# -*- coding: utf-8 -*-
"""Image_Classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19vviuuBIyNetsENvpeoOfO_pYq-ZTXmC

1. Main Project (70%)

a. For image classifier use the following dataset.
 https://www.kaggle.com/datasets/aryashah2k/mango-leaf-diseasedataset
"""

! unzip '/content/drive/MyDrive/Mango Leaf Desiease Detect/archive (12).zip'

"""**Import required libraries:**"""

import os
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from pathlib import Path
from tensorflow.keras.preprocessing import image_dataset_from_directory


import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from skimage.color import rgb2gray
from skimage.transform import resize

folder_names = [
    "Anthracnose",
    "Bacterial Canker",
    "Cutting Weevil",
    "Die Back",
    "Gall Midge",
    "Healthy",
    "Powdery Mildew",
    "Sooty Mould",
]

def filter_subdirs(subdirs):
    return [subdir for subdir in subdirs if Path(subdir).name in folder_names]

# Set dataset path and create ImageDataGenerator for data augmentation
data_dir = Path('/content')
train_datagen = ImageDataGenerator(rescale=1./255,
                                   rotation_range=40,
                                   width_shift_range=0.2,
                                   height_shift_range=0.2,
                                   shear_range=0.2,
                                   zoom_range=0.2,
                                   horizontal_flip=True,
                                   validation_split=0.2)

train_generator = train_datagen.flow_from_directory(data_dir,
                                                    target_size=(260, 260),
                                                    batch_size=32,
                                                    class_mode='categorical',
                                                    subset='training',
                                                    classes=folder_names)
validation_generator = train_datagen.flow_from_directory(data_dir,
                                                         target_size=(260, 260),
                                                         batch_size=32,
                                                         class_mode='categorical',
                                                         subset='validation',
                                                         classes=folder_names)

"""# EDA"""

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

"""**Create a function to count the number of images per class:**"""

def count_images_per_class(folder_names, data_dir):
    class_counts = {}
    for folder in folder_names:
        class_path = data_dir / folder
        num_images = len(list(class_path.glob('*.jpg')))  # assuming the images are in jpg format
        class_counts[folder] = num_images
    return class_counts

class_counts = count_images_per_class(folder_names, data_dir)
class_counts_df = pd.DataFrame.from_dict(class_counts, orient='index', columns=['count'])

plt.figure(figsize=(12, 6))
sns.barplot(data=class_counts_df.reset_index(), x='index', y='count')
plt.xlabel('Class')
plt.ylabel('Number of Images')
plt.title('Distribution of Images Across Classes')
plt.xticks(rotation=45)
plt.show()

from PIL import Image
import random

"""**Create a function to display sample images from each class:**"""

def display_sample_images(folder_names, data_dir, num_samples=3):
    fig, axes = plt.subplots(len(folder_names), num_samples, figsize=(12, 2 * len(folder_names)))
    
    for i, folder in enumerate(folder_names):
        class_path = data_dir / folder
        image_files = list(class_path.glob('*.jpg'))
        samples = random.sample(image_files, num_samples)
        
        for j, image_file in enumerate(samples):
            img = Image.open(image_file)
            axes[i, j].imshow(img)
            axes[i, j].set_axis_off()
            
            if j == 0:
                axes[i, j].set_title(folder)
    
    plt.subplots_adjust(wspace=0.05, hspace=0.25)
    plt.show()

display_sample_images(folder_names, data_dir)

"""**Create a function to compute the average image dimensions:**"""

def compute_average_dimensions(folder_names, data_dir):
    total_images = 0
    total_width = 0
    total_height = 0
    
    for folder in folder_names:
        class_path = data_dir / folder
        image_files = list(class_path.glob('*.jpg'))
        
        for image_file in image_files:
            img = Image.open(image_file)
            total_width += img.width
            total_height += img.height
            total_images += 1
    
    avg_width = total_width / total_images
    avg_height = total_height / total_images
    
    return avg_width, avg_height

"""**Compute the average image dimensions:**"""

avg_width, avg_height = compute_average_dimensions(folder_names, data_dir)
print(f"Average width: {avg_width:.2f}, Average height: {avg_height:.2f}")

"""# Model Building

**!. CNN**
"""

model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(260, 260, 3)),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(512, activation='relu'),
    Dropout(0.5),
    Dense(8, activation='softmax')  # Number of classes (8) for the mango leaf diseases
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

history = model.fit(train_generator,
                    steps_per_epoch=len(train_generator),
                    epochs=100,
                    validation_data=validation_generator,
                    validation_steps=len(validation_generator))

plt.plot(history.history['accuracy'], label='train_accuracy')
plt.plot(history.history['val_accuracy'], label='val_accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

model.save('/content/drive/MyDrive/Mango Leaf Desiease Detect/Best_Model.h5')

# model=tf.keras.models.load_model('/content/drive/MyDrive/Mango Leaf Desiease Detect/Best_Model.h5')

model.evaluate(validation_generator)

"""In comparison to the original model from the referenced paper, I made the following improvements that resulted in an increase in the test accuracy from 96% to 97.5%:

* Simplification: I simplified the architecture by removing unnecessary filter sizes and only used 3x3 convolutional filters throughout the model. This resulted in a more consistent and easier-to-understand architecture.

* Depth: I increased the depth of the model by adding an additional convolutional layer for the 128-filter block. This allowed the model to learn more complex and hierarchical features, which improved its overall performance.

* Pooling layers: I changed the max-pooling layer sizes to 2x2 in all layers. This provided a more balanced downsampling of the feature maps and enabled the model to retain more spatial information.

* Dropout: I introduced a dropout layer with a rate of 0.5 after the first fully connected layer. This regularized the model and helped prevent overfitting.

* Activation function: I consistently used the ReLU activation function in all convolutional layers, as it is less prone to vanishing gradients and generally results in faster convergence.

* Input size: I adjusted the input size of the model to 260x260x3 to better align with the actual input image size, which likely led to better performance due to a better match between the model input and the data.


In summary, the improvements made to the model architecture and training strategy allowed for better feature extraction and representation, leading to an increase in the model's test accuracy from 96% to 97.5%. These changes resulted in a more effective and robust classifier for the mango leaf disease dataset, ultimately enhancing the model's ability to identify and classify different diseases accurately.
"""

# Extract images and labels from the train_generator
images = []
labels = []
for i in range(len(train_generator)):
    batch_images, batch_labels = train_generator[i]
    images.extend(batch_images)
    labels.extend(np.argmax(batch_labels, axis=1))
    
# Convert images and labels to numpy arrays
images = np.array(images)
labels = np.array(labels)

# Flatten and convert to grayscale
X = [resize(rgb2gray(img), (260, 260)).flatten() for img in images]
y = labels

# Split the dataset into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale the data
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""# Other Machine Learning Models"""

# Define the models
models = {
    'SVM': SVC(),
    'KNN': KNeighborsClassifier(),
    'Random Forest': RandomForestClassifier()
}

# Train and evaluate the models
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')
    
    print(f"{name}:")
    print(f"  Accuracy: {accuracy:.4f}")
    print(f"  Precision: {precision:.4f}")
    print(f"  Recall: {recall:.4f}")
    print(f"  F1-score: {f1:.4f}")
    print()

"""c. Try to use as many as algorithms you learned in the class. Compare the pros
and cons of the algorithms.

In task C, you implemented three different machine learning algorithms: Support Vector Machine (SVM), K-Nearest Neighbors (KNN), and Random Forest. Below are the pros and cons of each algorithm based on their performance on the mango leaf disease dataset:

Support Vector Machine (SVM)
Pros:
Effective in high-dimensional spaces.
Good generalization capabilities, as it aims to maximize the margin between classes.
Cons:

Computationally expensive, especially for large datasets.
Requires proper tuning of the kernel and hyperparameters for optimal performance.
Accuracy: 0.4672
Precision: 0.4764
Recall: 0.4672
F1-score: 0.4635

K-Nearest Neighbors (KNN)
Pros:
Simple and easy to implement.
Non-parametric, meaning it doesn't make any assumptions about the underlying data distribution.
Cons:

Computationally expensive during prediction, as it searches for the nearest neighbors.
Sensitive to the choice of the hyperparameter K and the distance metric.
Accuracy: 0.3344
Precision: 0.4100
Recall: 0.3344
F1-score: 0.3054

Random Forest
Pros:
Robust to overfitting due to its ensemble nature.
Can handle large datasets and high-dimensional features.
Good for both classification and regression tasks.
Cons:

Can be slower to train and predict compared to simpler models.
Interpretability may be more challenging due to the ensemble of decision trees.
Accuracy: 0.6219
Precision: 0.6247
Recall: 0.6219
F1-score: 0.6214

Based on the results, the Random Forest classifier outperforms the SVM and KNN classifiers in terms of accuracy, precision, recall, and F1-score for this specific dataset. It is important to note that these results might change with different datasets or different parameter settings for each algorithm.
"""

